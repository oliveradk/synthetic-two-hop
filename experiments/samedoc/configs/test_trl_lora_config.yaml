learning_rate: 0.0002  # Higher learning rate typical for LoRA
max_steps: 1  # Override num_train_epochs with just 1 step
per_device_train_batch_size: 1  # Batch size of 1 to avoid OOM
gradient_accumulation_steps: 1  # No accumulation for quick test
max_grad_norm: 1
seed: 42
bf16: true
save_only_model: true
logging_steps: 1
eval_strategy: "steps"
eval_steps: 1  # Evaluate after every step
save_strategy: "no"  # Don't save checkpoints for test run
push_to_hub: false
disable_tqdm: false  # Keep progress bar for visibility
output_dir: models/test_run
eval_on_start: true
report_to: "none"  # Disable wandb for test run

# LoRA specific configuration
use_peft: true
peft_config:
  peft_type: "LORA"
  r: 8  # LoRA rank
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  task_type: "CAUSAL_LM"